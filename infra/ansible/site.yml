- hosts: gcp
  become: true

  vars:
    # ---- inputs (override via -e) ----
    k8s_ns: "{{ K8S_NS | default('music') }}"
    host: "{{ HOST | default('music.' + lookup('env','GCP_IP') + '.sslip.io') }}"
    image: "{{ IMAGE | default('ghcr.io/jorjd666/yt-llm-music-suggester:latest') }}"
    yt_key: "{{ YT_KEY | default('') }}"
    openai_key: "{{ OPENAI_KEY | default('') }}"
    api_token: "{{ API_TOKEN | default('') }}"
    expose_via: "{{ EXPOSE_VIA | default('ingress') }}"     # 'ingress' or 'nodeport'
    nodeport: 30080
    acme_email: "{{ ACME_EMAIL | default('') }}"            # e.g. george.duimovici@gmail.com
    issuer_env: "{{ ISSUER_ENV | default('staging') }}"     # 'staging' or 'production'
    manage_cert_manager: "{{ MANAGE_CERT_MANAGER | default(true) }}"
    monitoring_ns: "{{ MONITORING_NS | default('monitoring') }}"
    grafana_host: "{{ GRAFANA_HOST | default('grafana.' + lookup('env','GCP_IP') + '.sslip.io') }}"
    manage_monitoring: "{{ MANAGE_MONITORING | default(true) }}"

    # Ollama controls
    enable_ollama: "{{ ENABLE_OLLAMA | default(true) }}"
    ollama_model: "{{ OLLAMA_MODEL | default('llama3.1:8b') }}"

    # ---- derived ----
    issuer_name: >-
      {{ 'letsencrypt-production' if issuer_env == 'production' else 'letsencrypt-staging' }}
    acme_server: >-
      {{ 'https://acme-v02.api.letsencrypt.org/directory'
         if issuer_env == 'production'
         else 'https://acme-staging-v02.api.letsencrypt.org/directory' }}

  pre_tasks:
    - name: Force noninteractive apt
      copy:
        dest: /etc/apt/apt.conf.d/90noninteractive
        content: |
          APT::Get::Assume-Yes "true";
          APT::Get::force-yes "true";
          Dpkg::Options {
            "--force-confdef";
            "--force-confold";
          }
      tags: [k3s, app, ingress, swap]

    - name: Prefer IPv4 for apt (avoid IPv6 resolver issues)
      copy:
        dest: /etc/apt/apt.conf.d/99force-ipv4
        content: |
          Acquire::ForceIPv4 "true";
      tags: [k3s, app, ingress, swap]

    - name: Clear apt/dpkg locks if present (safe no-op if none)
      shell: |
        set -euxo pipefail
        rm -f /var/lib/apt/lists/lock || true
        rm -f /var/cache/apt/archives/lock || true
        rm -f /var/lib/dpkg/lock-frontend || true
        dpkg --configure -a || true
      args:
        executable: /bin/bash
      tags: [k3s, app, ingress, swap]

    - name: Ensure apt cache updated (with retries)
      apt:
        update_cache: yes
        force_apt_get: yes
        cache_valid_time: 3600
      register: apt_update
      retries: 6
      delay: 10
      until: apt_update is succeeded
      tags: [k3s, app, ingress, swap]

  tasks:
    # ---------- system prep ----------
    - name: Add 2G swap (idempotent)
      shell: |
        if ! grep -q "/swapfile" /etc/fstab; then
          fallocate -l 2G /swapfile || dd if=/dev/zero of=/swapfile bs=1M count=2048
          chmod 600 /swapfile
          mkswap /swapfile
          swapon /swapfile
          echo '/swapfile none swap sw 0 0' >> /etc/fstab
          echo 'vm.swappiness=10' > /etc/sysctl.d/99-swappiness.conf
          sysctl --system
        fi
      args:
        executable: /bin/bash
      tags: [swap]

    - name: Install base tools
      apt:
        name: [curl, git, ca-certificates]
        state: present
      tags: [k3s]

    # ---------- Ollama on host ----------
    - name: Install Ollama (if enabled)
      when: enable_ollama | bool
      shell: |
        curl -fsSL https://ollama.com/install.sh | sh
      args:
        executable: /bin/bash
        creates: /usr/local/bin/ollama
      tags: [ollama]

    - name: Ensure Ollama service enabled and started
      when: enable_ollama | bool
      systemd:
        name: ollama
        enabled: true
        state: started
      tags: [ollama]

    - name: Ensure Ollama config directory exists
      when: enable_ollama | bool
      file:
        path: /etc/ollama
        state: directory
        owner: root
        group: root
        mode: '0755'
      tags: [ollama]

    - name: Configure Ollama resource limits
      when: enable_ollama | bool
      copy:
        dest: /etc/ollama/config.yaml
        owner: root
        group: root
        mode: '0644'
        content: |
          # Autogenerated by Ansible â€“ tuned for 2 vCPU / 4GB
          num_ctx: 2048
          num_thread: 2
          max_loaded_models: 1
      notify: Restart ollama
      tags: [ollama]

    - name: Pull Ollama model
      when: enable_ollama | bool
      shell: |
        ollama pull {{ ollama_model }}
      args:
        executable: /bin/bash
      tags: [ollama]

    # ---------- k3s ----------
    - name: Install k3s (Traefik enabled)
      shell: |
        curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644" sh -s -
      args:
        creates: /usr/local/bin/k3s
      register: k3s_install
      changed_when: "'Installed K3s' in k3s_install.stdout or k3s_install.rc == 0"
      tags: [k3s]

    - name: Wait for k3s systemd service to be active
      shell: systemctl is-active --quiet k3s
      register: k3s_svc
      retries: 40
      delay: 5
      until: k3s_svc.rc == 0
      tags: [k3s]

    - name: Wait for node to be Ready
      shell: |
        k3s kubectl get nodes -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}'
      register: node_ready
      retries: 40
      delay: 5
      until: node_ready.stdout == "True"
      tags: [k3s]

    - name: Wait for kube-apiserver to respond
      shell: k3s kubectl get ns
      register: api_ready
      retries: 40
      delay: 5
      until: api_ready.rc == 0
      tags: [k3s]

    - name: Create namespace
      shell: k3s kubectl create ns {{ k8s_ns }} || true
      tags: [app, ingress]

    # ---------- (optional) cert-manager via Helm ----------
    - name: Install Helm (if missing)
      when: manage_cert_manager | bool or manage_monitoring | bool
      shell: |
        if ! command -v helm >/dev/null 2>&1; then
          curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        fi
      args:
        executable: /bin/bash
      tags: [ingress, monitoring]

    - name: Add jetstack repo
      when: manage_cert_manager | bool
      shell: helm repo add jetstack https://charts.jetstack.io || true
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      tags: [ingress]

    - name: Helm repo update (cert-manager)
      when: manage_cert_manager | bool
      shell: helm repo update
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      tags: [ingress]

    - name: Install/upgrade cert-manager (CRDs enabled)
      when: manage_cert_manager | bool
      shell: |
        helm upgrade --install cert-manager jetstack/cert-manager \
          --namespace cert-manager --create-namespace \
          --set crds.enabled=true
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      register: helm_certmgr
      retries: 3
      delay: 30
      until: helm_certmgr.rc == 0
      tags: [ingress]

    - name: Wait for cert-manager pods Ready
      when: manage_cert_manager | bool
      shell: |
        k3s kubectl -n cert-manager rollout status deploy/cert-manager --timeout=180s && \
        k3s kubectl -n cert-manager rollout status deploy/cert-manager-webhook --timeout=180s && \
        k3s kubectl -n cert-manager rollout status deploy/cert-manager-cainjector --timeout=180s
      tags: [ingress]

    - name: Create ClusterIssuer (staging)
      when: manage_cert_manager | bool
      copy:
        dest: /tmp/clusterissuer-staging.yaml
        content: |
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-staging
          spec:
            acme:
              email: "{{ acme_email | default('george.duimovici@gmail.com') }}"
              server: https://acme-staging-v02.api.letsencrypt.org/directory
              privateKeySecretRef:
                name: letsencrypt-staging-key
              solvers:
              - http01:
                  ingress:
                    class: traefik
      tags: [ingress]

    - name: Apply ClusterIssuer (staging)
      when: manage_cert_manager | bool
      shell: k3s kubectl apply -f /tmp/clusterissuer-staging.yaml
      tags: [ingress]

    - name: Create ClusterIssuer (production)
      when: manage_cert_manager | bool
      copy:
        dest: /tmp/clusterissuer-production.yaml
        content: |
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: letsencrypt-production
          spec:
            acme:
              email: "{{ acme_email | default('george.duimovici@gmail.com') }}"
              server: https://acme-v02.api.letsencrypt.org/directory
              privateKeySecretRef:
                name: letsencrypt-production-key
              solvers:
              - http01:
                  ingress:
                    class: traefik
      tags: [ingress]

    - name: Apply ClusterIssuer (production)
      when: manage_cert_manager | bool
      shell: k3s kubectl apply -f /tmp/clusterissuer-production.yaml
      tags: [ingress]

    # ---------- monitoring: kube-prometheus-stack (Prometheus + Alertmanager + Grafana) ----------
    - name: Ensure monitoring namespace exists
      when: manage_monitoring | bool
      shell: k3s kubectl create ns {{ monitoring_ns }} || true
      tags: [monitoring]

    - name: Add prometheus-community repo
      when: manage_monitoring | bool
      shell: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      tags: [monitoring]

    - name: Helm repo update (monitoring)
      when: manage_monitoring | bool
      shell: helm repo update
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      tags: [monitoring]

    - name: Install/upgrade kube-prometheus-stack (Grafana + Alertmanager)
      when: manage_monitoring | bool
      shell: |
        helm upgrade --install kps prometheus-community/kube-prometheus-stack \
          --namespace {{ monitoring_ns }} --create-namespace \
          --set grafana.service.type=ClusterIP \
          --set grafana.ingress.enabled=true \
          --set grafana.ingress.ingressClassName=traefik \
          --set grafana.ingress.annotations."kubernetes\.io/ingress\.class"=traefik \
          --set grafana.ingress.annotations."cert-manager\.io/cluster-issuer"="{{ issuer_name }}" \
          --set grafana.ingress.hosts[0]="{{ grafana_host }}" \
          --set grafana.ingress.tls[0].hosts[0]="{{ grafana_host }}" \
          --set grafana.ingress.tls[0].secretName="grafana-tls"
      environment:
        KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      register: helm_kps
      retries: 3
      delay: 60
      until: helm_kps.rc == 0
      tags: [monitoring]

    - name: Wait for Grafana deployment Ready
      when: manage_monitoring | bool
      shell: k3s kubectl -n {{ monitoring_ns }} rollout status deploy/kps-grafana --timeout=300s
      tags: [monitoring]

    - name: Create ServiceMonitor for yt-llm
      when: manage_monitoring | bool
      copy:
        dest: /tmp/yt-llm-servicemonitor.yaml
        content: |
          apiVersion: monitoring.coreos.com/v1
          kind: ServiceMonitor
          metadata:
            name: yt-llm-servicemonitor
            namespace: {{ monitoring_ns }}
            labels:
              release: kps
          spec:
            selector:
              matchLabels:
                app: yt-llm
            namespaceSelector:
              matchNames:
                - {{ k8s_ns }}
            endpoints:
              - port: http
                path: /metrics
                interval: 30s
      tags: [monitoring]

    - name: Apply ServiceMonitor for yt-llm
      when: manage_monitoring | bool
      shell: k3s kubectl apply -f /tmp/yt-llm-servicemonitor.yaml
      tags: [monitoring]

    # ---------- app config ----------
    - name: Pre-pull app image (containerd) to avoid slow first pull
      shell: k3s ctr images pull {{ image }}
      register: pull_img
      failed_when: false
      changed_when: "'resolved' in pull_img.stdout or 'exists' in pull_img.stdout"
      tags: [app]

    - name: Apply ConfigMap
      copy:
        dest: /tmp/cm.yaml
        content: |
          apiVersion: v1
          kind: ConfigMap
          metadata: { name: yt-llm-config, namespace: {{ k8s_ns }} }
          data:
            LLM_PROVIDER: "openai"
            OPENAI_BASE_URL: "http://{{ ansible_default_ipv4.address }}:11434/v1"
            OPENAI_MODEL: "{{ ollama_model }}"
            RATE_LIMIT: "10/minute"
            MAX_YT_RESULTS: "25"
            MAX_SUGGESTIONS: "10"
            REQUESTS_TIMEOUT: "10"
      tags: [app]

    - name: kubectl apply cm (no validation)
      shell: k3s kubectl apply -f /tmp/cm.yaml --validate=false
      tags: [app]

    - name: Apply Secrets
      copy:
        dest: /tmp/secret.yaml
        content: |
          apiVersion: v1
          kind: Secret
          metadata: { name: yt-llm-secrets, namespace: {{ k8s_ns }} }
          type: Opaque
          stringData:
            YOUTUBE_API_KEY: "{{ yt_key }}"
            OPENAI_API_KEY: "{{ openai_key }}"
            API_TOKEN: "{{ api_token }}"
      no_log: true
      tags: [app]

    - name: kubectl apply secrets (no validation)
      shell: k3s kubectl apply -f /tmp/secret.yaml --validate=false
      register: secrets_apply
      tags: [app]

    - name: Apply Deployment+Service (light resources)
      copy:
        dest: /tmp/app.yaml
        content: |
          apiVersion: apps/v1
          kind: Deployment
          metadata: { name: yt-llm, namespace: {{ k8s_ns }} }
          spec:
            replicas: 1
            selector: { matchLabels: { app: yt-llm } }
            template:
              metadata: { labels: { app: yt-llm } }
              spec:
                containers:
                - name: api
                  image: {{ image }}
                  ports: [{ containerPort: 8000 }]
                  envFrom:
                  - configMapRef: { name: yt-llm-config }
                  - secretRef:    { name: yt-llm-secrets }
                  readinessProbe: { httpGet: { path: /healthz, port: 8000 }, initialDelaySeconds: 5, periodSeconds: 10 }
                  livenessProbe:  { httpGet: { path: /healthz, port: 8000 }, initialDelaySeconds: 10, periodSeconds: 20 }
                  resources:
                    requests: { cpu: "50m", memory: "64Mi" }
                    limits:   { cpu: "200m", memory: "256Mi" }
                  securityContext:
                    runAsNonRoot: true
                    runAsUser: 10001
                    allowPrivilegeEscalation: false
                    readOnlyRootFilesystem: true
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: yt-llm-svc
            namespace: {{ k8s_ns }}
            labels: { app: yt-llm }
            annotations:
              prometheus.io/scrape: "true"
              prometheus.io/path: "/metrics"
              prometheus.io/port: "8000"
          spec:
            selector: { app: yt-llm }
            ports:
              - name: http
                port: 80
                targetPort: 8000
      tags: [app]

    - name: kubectl apply app (no validation)
      shell: k3s kubectl apply -f /tmp/app.yaml --validate=false
      register: app_apply
      tags: [app]

    # ---------- refresh pods when Secret changed ----------
    - name: Restart deployment if Secret changed (pick up new API_TOKEN)
      when: secrets_apply is changed
      shell: k3s kubectl -n {{ k8s_ns }} rollout restart deploy/yt-llm
      tags: [app]

    - name: Wait for rollout
      shell: k3s kubectl -n {{ k8s_ns }} rollout status deploy/yt-llm
      register: rollout
      retries: 30
      delay: 6
      until: rollout.rc == 0
      tags: [app]

    # ---------- exposure ----------
    - name: Write Ingress manifest (Traefik) with TLS + issuer
      when: expose_via == 'ingress'
      copy:
        dest: /tmp/ing.yaml
        content: |
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: yt-llm-ing
            namespace: {{ k8s_ns }}
            annotations:
              kubernetes.io/ingress.class: "traefik"
              cert-manager.io/cluster-issuer: "{{ issuer_name }}"
          spec:
            ingressClassName: traefik
            tls:
            - hosts: ["{{ host }}"]
              secretName: yt-llm-tls
            rules:
            - host: "{{ host }}"
              http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: yt-llm-svc
                      port:
                        number: 80
      tags: [ingress]

    - name: kubectl apply ingress (no validation)
      when: expose_via == 'ingress'
      shell: k3s kubectl apply -f /tmp/ing.yaml --validate=false
      tags: [ingress]

    - name: Expose via NodePort when expose_via=nodeport
      when: expose_via == 'nodeport'
      shell: |
        k3s kubectl -n {{ k8s_ns }} patch svc yt-llm-svc --type='merge' -p \
        '{"spec":{"type":"NodePort","ports":[{"name":"http","port":80,"targetPort":8000,"nodePort":{{ nodeport }} }]}}'
      tags: [ingress]

    - name: Show endpoints
      shell: |
        echo "App:     https://{{ host }}/"
        echo "Health:  https://{{ host }}/healthz"
        echo "Metrics: https://{{ host }}/metrics"
        echo "Grafana: https://{{ grafana_host }}/"
        k3s kubectl -n {{ k8s_ns }} get pods,svc,ingress -o wide || true
        k3s kubectl -n {{ monitoring_ns }} get pods,svc,ingress -o wide || true
      register: ep
      tags: [app, ingress, monitoring]

    - debug: var=ep.stdout
      tags: [app, ingress, monitoring]

  handlers:
    - name: Restart ollama
      service:
        name: ollama
        state: restarted
